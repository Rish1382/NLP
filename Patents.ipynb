{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9045066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79861258",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"~/Downloads/train.csv\")\n",
    "test = pd.read_csv(\"~/Downloads/test.csv\")\n",
    "\n",
    "combined = pd.concat([train, test], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16b52e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84456, 2), (59119, 2), (25337, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.shape, train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b5346a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>second counting input 5 2 which receives inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>extremely low temperature of the chips in cold...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>of the basic ammonium salt of the carboxyl ate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>18 u2033 is provided which is axially supporte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>to an u201c inner surface u201d means the surf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                            Content\n",
       "0    2.0  second counting input 5 2 which receives inter...\n",
       "1    4.0  extremely low temperature of the chips in cold...\n",
       "2    3.0  of the basic ammonium salt of the carboxyl ate...\n",
       "3    9.0  18 u2033 is provided which is axially supporte...\n",
       "4    2.0  to an u201c inner surface u201d means the surf..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd132187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets apply the function....\n",
    "import re\n",
    "def clean_text(text):\n",
    "    text = re.sub(\"(www.+)|(\\s+)|(@[a-zA-Z]+)|\\W+\", \" \", text) # removes hyperlinks, special chars\n",
    "    text = re.sub('(\\w+:/\\S+)', \" \", text)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(\"http|https\", \" \", text)\n",
    "    text = re.sub(\"[^a-zA-Z0-9]+\", \" \", text)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6148d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined[\"Clean_Context\"] = combined.Content.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad21e7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Content</th>\n",
       "      <th>Clean_Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>second counting input 5 2 which receives inter...</td>\n",
       "      <td>second counting input 5 2 which receives inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>extremely low temperature of the chips in cold...</td>\n",
       "      <td>extremely low temperature of the chips in cold...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>of the basic ammonium salt of the carboxyl ate...</td>\n",
       "      <td>of the basic ammonium salt of the carboxyl ate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>18 u2033 is provided which is axially supporte...</td>\n",
       "      <td>18 u2033 is provided which is axially supporte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>to an u201c inner surface u201d means the surf...</td>\n",
       "      <td>to an u201c inner surface u201d means the surf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                            Content  \\\n",
       "0    2.0  second counting input 5 2 which receives inter...   \n",
       "1    4.0  extremely low temperature of the chips in cold...   \n",
       "2    3.0  of the basic ammonium salt of the carboxyl ate...   \n",
       "3    9.0  18 u2033 is provided which is axially supporte...   \n",
       "4    2.0  to an u201c inner surface u201d means the surf...   \n",
       "\n",
       "                                       Clean_Context  \n",
       "0  second counting input 5 2 which receives inter...  \n",
       "1  extremely low temperature of the chips in cold...  \n",
       "2  of the basic ammonium salt of the carboxyl ate...  \n",
       "3  18 u2033 is provided which is axially supporte...  \n",
       "4  to an u201c inner surface u201d means the surf...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cb81709",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined[\"text_len\"] = combined.Clean_Context.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbfa35c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Content</th>\n",
       "      <th>Clean_Context</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>second counting input 5 2 which receives inter...</td>\n",
       "      <td>second counting input 5 2 which receives inter...</td>\n",
       "      <td>1504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>extremely low temperature of the chips in cold...</td>\n",
       "      <td>extremely low temperature of the chips in cold...</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>of the basic ammonium salt of the carboxyl ate...</td>\n",
       "      <td>of the basic ammonium salt of the carboxyl ate...</td>\n",
       "      <td>1345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>18 u2033 is provided which is axially supporte...</td>\n",
       "      <td>18 u2033 is provided which is axially supporte...</td>\n",
       "      <td>1569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>to an u201c inner surface u201d means the surf...</td>\n",
       "      <td>to an u201c inner surface u201d means the surf...</td>\n",
       "      <td>1733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                            Content  \\\n",
       "0    2.0  second counting input 5 2 which receives inter...   \n",
       "1    4.0  extremely low temperature of the chips in cold...   \n",
       "2    3.0  of the basic ammonium salt of the carboxyl ate...   \n",
       "3    9.0  18 u2033 is provided which is axially supporte...   \n",
       "4    2.0  to an u201c inner surface u201d means the surf...   \n",
       "\n",
       "                                       Clean_Context  text_len  \n",
       "0  second counting input 5 2 which receives inter...      1504  \n",
       "1  extremely low temperature of the chips in cold...      1251  \n",
       "2  of the basic ammonium salt of the carboxyl ate...      1345  \n",
       "3  18 u2033 is provided which is axially supporte...      1569  \n",
       "4  to an u201c inner surface u201d means the surf...      1733  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e10af3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "newtrain = combined.loc[0:train.shape[0]-1, :]\n",
    "newtest = combined.loc[train.shape[0]:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "020415a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "newtest.drop(\"Label\", axis  = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e58bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Assume 'texts' is a list containing your documents\n",
    "\n",
    "\n",
    "# Tokenization using Gensim\n",
    "tokenized_train_docs = [simple_preprocess(text, deacc=True) for text in newtrain['Clean_Context']]  # `deacc=True` removes punctuations\n",
    "\n",
    "# Tokenization using Gensim\n",
    "tokenized_test_docs = [simple_preprocess(text, deacc=True) for text in newtest['Clean_Context']]  # `deacc=True` removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b7136f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# ... your code for training the Word2Vec model ...\n",
    "word2vec_model = Word2Vec(sentences=tokenized_test_docs,\n",
    "                          vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "\n",
    "def document_vector(word2vec_model, doc):\n",
    "    # Remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.wv.key_to_index]\n",
    "    if len(doc) == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    # Average the word vectors for a document\n",
    "    return np.mean(word2vec_model.wv[doc], axis=0)\n",
    "\n",
    "# Assuming `tokenized_train_docs` is a list of tokenized documents\n",
    "vector_train = np.array([document_vector(word2vec_model, doc) for doc in tokenized_train_docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "91310031",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, vector in enumerate(vector_test):\n",
    "    if isinstance(vector, np.ndarray):\n",
    "        continue\n",
    "    elif isinstance(vector, np.float64):\n",
    "        print(f\"Index {i} is a float, not an array.\")\n",
    "    else:\n",
    "        print(f\"Index {i} has type {type(vector)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eb13016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you know the dimensionality of your word vectors, e.g., 100\n",
    "vector_size = 100  # replace with the correct size of your vectors\n",
    "vectors_train_corrected = np.zeros((len(vectors_train), vector_size))\n",
    "\n",
    "for i, vector in enumerate(vectors_train):\n",
    "    if isinstance(vector, np.float64):\n",
    "        vectors_train_corrected[i] = np.zeros(vector_size)  # a zero vector\n",
    "    else:\n",
    "        vectors_train_corrected[i] = vector  # the original vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53ae3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Word2Vec model on Train Set\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=tokenized_test_docs,\n",
    "                          vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def document_vector(word2vec_model, doc):\n",
    "    # Remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.wv.key_to_index]\n",
    "    if len(doc) == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    # Average the word vectors for a document\n",
    "    return np.mean(word2vec_model.wv[doc], axis=0)\n",
    "    \n",
    "# Average the word vectors for a document\n",
    "vector_test = np.array([document_vector(word2vec_model, doc) for doc in tokenized_test_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0643bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "newtrain['Label'] = newtrain['Label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c6d4acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors_train_corrected, newtrain['Label'], \n",
    "                                                    test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Train a classifier\n",
    "classifier = GradientBoostingClassifier()\n",
    "predictions = classifier.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7cccae80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.37      0.40      1356\n",
      "           2       0.27      0.26      0.27      1322\n",
      "           3       0.55      0.77      0.65      1221\n",
      "           4       0.59      0.52      0.55      1271\n",
      "           5       0.43      0.53      0.48      1276\n",
      "           6       0.43      0.55      0.48      1331\n",
      "           7       0.48      0.54      0.50      1398\n",
      "           8       0.56      0.59      0.57      1449\n",
      "           9       0.24      0.04      0.06      1200\n",
      "\n",
      "    accuracy                           0.47     11824\n",
      "   macro avg       0.44      0.46      0.44     11824\n",
      "weighted avg       0.44      0.47      0.44     11824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "474f6e6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Clean_Content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\notebook\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Clean_Content'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Perform TF-IDF vectorization\u001b[39;00m\n\u001b[0;32m     10\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)  \u001b[38;5;66;03m# Limit the number of features to 1000 for simplicity\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m X_tfidf \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mnewtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mClean_Content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     14\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[0;32m     15\u001b[0m y_encoded \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(newtrain[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\notebook\\lib\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\notebook\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Clean_Content'"
     ]
    }
   ],
   "source": [
    "# Encode the labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Perform TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Limit the number of features to 1000 for simplicity\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(newtrain['Clean_Content']).toarray()\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(newtrain['Label'])\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "y_train_encoded = to_categorical(y_train)\n",
    "y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the neural network structure\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))  # The output layer size should match the number of labels\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4b66794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mukul\\anaconda3\\envs\\notebook\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\mukul\\anaconda3\\envs\\notebook\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mukul\\anaconda3\\envs\\notebook\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "666/666 [==============================] - 2s 2ms/step - loss: 1.5447 - accuracy: 0.4483 - val_loss: 1.4663 - val_accuracy: 0.4742\n",
      "Epoch 2/10\n",
      "666/666 [==============================] - 1s 2ms/step - loss: 1.4424 - accuracy: 0.4866 - val_loss: 1.5139 - val_accuracy: 0.4666\n",
      "Epoch 3/10\n",
      "666/666 [==============================] - 1s 2ms/step - loss: 1.4090 - accuracy: 0.4996 - val_loss: 1.4456 - val_accuracy: 0.4763\n",
      "Epoch 4/10\n",
      "666/666 [==============================] - 1s 2ms/step - loss: 1.3825 - accuracy: 0.5098 - val_loss: 1.4081 - val_accuracy: 0.4983\n",
      "Epoch 5/10\n",
      "666/666 [==============================] - 1s 2ms/step - loss: 1.3652 - accuracy: 0.5123 - val_loss: 1.4197 - val_accuracy: 0.4888\n",
      "Epoch 6/10\n",
      "666/666 [==============================] - 1s 2ms/step - loss: 1.3456 - accuracy: 0.5205 - val_loss: 1.4002 - val_accuracy: 0.4989\n",
      "Epoch 7/10\n",
      "666/666 [==============================] - 1s 2ms/step - loss: 1.3279 - accuracy: 0.5282 - val_loss: 1.4017 - val_accuracy: 0.5038\n",
      "Epoch 8/10\n",
      "666/666 [==============================] - 1s 2ms/step - loss: 1.3121 - accuracy: 0.5304 - val_loss: 1.4021 - val_accuracy: 0.5074\n",
      "Epoch 9/10\n",
      "666/666 [==============================] - 1s 2ms/step - loss: 1.2955 - accuracy: 0.5369 - val_loss: 1.3939 - val_accuracy: 0.5061\n",
      "Epoch 10/10\n",
      "666/666 [==============================] - 1s 2ms/step - loss: 1.2774 - accuracy: 0.5441 - val_loss: 1.4077 - val_accuracy: 0.5047\n",
      "370/370 [==============================] - 0s 881us/step - loss: 1.4084 - accuracy: 0.5043\n",
      "Test Accuracy: 50.43%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "# Use y_train_encoded.shape[1] to determine the output layer size\n",
    "model.add(Dense(y_train_encoded.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Use y_train_encoded here\n",
    "model.fit(X_train, y_train_encoded, epochs=10, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Use y_test_encoded to evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f'Test Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a11ced78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47295,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed5e95a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
